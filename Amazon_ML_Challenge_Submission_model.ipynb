{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arindaym1/Entity-Information-ML-project/blob/main/Amazon_ML_Challenge_Submission_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install easyocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpv9WFgh8CKb",
        "outputId": "0dbd8fe3-37c4-4341-d20c-adcbc2c4b911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting easyocr\n",
            "  Downloading easyocr-1.7.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.19.0+cu121)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from easyocr) (4.10.0.84)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from easyocr) (9.4.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.23.2)\n",
            "Collecting python-bidi (from easyocr)\n",
            "  Downloading python_bidi-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from easyocr) (6.0.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.0.6)\n",
            "Collecting pyclipper (from easyocr)\n",
            "  Downloading pyclipper-1.3.0.post5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting ninja (from easyocr)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (2024.6.1)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2.34.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2024.8.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->easyocr) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->easyocr) (1.3.0)\n",
            "Downloading easyocr-1.7.1-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.3.0.post5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (908 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_bidi-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-bidi, pyclipper, ninja, easyocr\n",
            "Successfully installed easyocr-1.7.1 ninja-1.11.1.1 pyclipper-1.3.0.post5 python-bidi-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import easyocr\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Input, Concatenate, Lambda, GlobalAveragePooling2D, GlobalAveragePooling1D, Flatten, Embedding\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "JlBMQhgAS5GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "train_df = pd.read_csv('/content/train.csv')"
      ],
      "metadata": {
        "id": "iS8D8QKKS5Ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Balance the dataset\n",
        "def balance_sample(df, sample_size=1500):\n",
        "    group_id_sample = df.groupby('group_id').apply(lambda x: x.sample(n=min(len(x), sample_size // len(df['group_id'].unique())), random_state=42))\n",
        "    entity_type_sample = df.groupby('entity_name').apply(lambda x: x.sample(n=min(len(x), sample_size // len(df['entity_name'].unique())), random_state=42))\n",
        "\n",
        "    balanced_sample = pd.concat([group_id_sample, entity_type_sample]).drop_duplicates().sample(n=sample_size, random_state=42, replace=True)\n",
        "    return balanced_sample\n",
        "\n",
        "train_df = balance_sample(train_df)"
      ],
      "metadata": {
        "id": "_DnIDttBS5Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Download images based on image URLs\n",
        "drive_image_dir = '/content/images'\n",
        "os.makedirs(drive_image_dir, exist_ok=True)\n",
        "\n",
        "def download_image(url, file_path):\n",
        "    try:\n",
        "        img_data = requests.get(url).content\n",
        "        with open(file_path, 'wb') as handler:\n",
        "            handler.write(img_data)\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {url}: {e}\")\n",
        "\n",
        "for i, row in tqdm(train_df.iterrows(), total=train_df.shape[0]):\n",
        "    image_url = row['image_link']\n",
        "    image_name = image_url.split('/')[-1]\n",
        "    image_path = os.path.join(drive_image_dir, f\"{row['group_id']}_{image_name}\")\n",
        "    if not os.path.exists(image_path):\n",
        "        download_image(image_url, image_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5kg-zVlS5M4",
        "outputId": "e1677610-26ce-4faf-a261-a396767a267a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1500/1500 [00:19<00:00, 78.10it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Extract text from images using EasyOCR\n",
        "reader = easyocr.Reader(['en'])\n",
        "image_directory = '/content/images'\n",
        "\n",
        "def extract_text_from_image(group_id, image_name):\n",
        "    filename = f\"{group_id}_{image_name}\"\n",
        "    image_path = os.path.join(image_directory, filename)\n",
        "    # Check if the image file exists\n",
        "    if not os.path.exists(image_path):\n",
        "        print(f\"Image file not found: {image_path}\")\n",
        "        return \"\"\n",
        "\n",
        "    try:\n",
        "        result = reader.readtext(image_path)\n",
        "        text = ' '.join([res[1] for res in result])\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading image {image_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "train_df['image_name'] = train_df['image_link'].apply(lambda x: x.split('/')[-1])\n",
        "train_df['extracted_text'] = train_df.apply(lambda row: extract_text_from_image(row['group_id'], row['image_name']), axis=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8nH5yGbS5PQ",
        "outputId": "d8ca87c2-7f0c-4471-c211-1c26c8164aa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/easyocr/detection.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n",
            "/usr/local/lib/python3.10/dist-packages/easyocr/recognition.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Parse and normalize numeric values\n",
        "import re\n",
        "def parse_entity_value(value):\n",
        "    match = re.match(r'([0-9.]+)\\s*([a-zA-Z]+)', value)\n",
        "    if match:\n",
        "        numeric_value = float(match.group(1))\n",
        "        unit = match.group(2)\n",
        "        return pd.Series([numeric_value, unit])\n",
        "    return pd.Series([np.nan, np.nan])\n",
        "\n",
        "train_df[['numeric_value', 'unit']] = train_df['entity_value'].apply(parse_entity_value)\n",
        "train_df.dropna(subset=['numeric_value', 'unit'], inplace=True)\n",
        "\n",
        "# Normalize numeric values using MinMaxScaler\n",
        "numeric_scaler = MinMaxScaler()\n",
        "train_df['numeric_value_normalized'] = numeric_scaler.fit_transform(train_df[['numeric_value']])\n"
      ],
      "metadata": {
        "id": "v-jcC75oS5RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Encode categorical features\n",
        "group_id_encoder = LabelEncoder()\n",
        "entity_name_encoder = LabelEncoder()\n",
        "\n",
        "train_df['group_id_encoded'] = group_id_encoder.fit_transform(train_df['group_id'])\n",
        "train_df['entity_name_encoded'] = entity_name_encoder.fit_transform(train_df['entity_name'])\n"
      ],
      "metadata": {
        "id": "FRQl9FwrS5Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Constrain unit predictions based on entity_name\n",
        "entity_unit_map = {\n",
        "    \"width\": {\"centimetre\", \"foot\", \"millimetre\", \"metre\", \"inch\", \"yard\"},\n",
        "    \"depth\": {\"centimetre\", \"foot\", \"millimetre\", \"metre\", \"inch\", \"yard\"},\n",
        "    \"height\": {\"centimetre\", \"foot\", \"millimetre\", \"metre\", \"inch\", \"yard\"},\n",
        "    \"item_weight\": {\"milligram\", \"kilogram\", \"microgram\", \"gram\", \"ounce\", \"ton\", \"pound\"},\n",
        "    \"maximum_weight_recommendation\": {\"milligram\", \"kilogram\", \"microgram\", \"gram\", \"ounce\", \"ton\", \"pound\"},\n",
        "    \"voltage\": {\"millivolt\", \"kilovolt\", \"volt\"},\n",
        "    \"wattage\": {\"kilowatt\", \"watt\"},\n",
        "    \"item_volume\": {\"cubic foot\", \"microlitre\", \"cup\", \"fluid ounce\", \"centilitre\", \"imperial gallon\", \"pint\",\n",
        "                    \"decilitre\", \"litre\", \"millilitre\", \"quart\", \"cubic inch\", \"gallon\"}\n",
        "}\n",
        "\n",
        "unit_labels = sorted(set(unit for units in entity_unit_map.values() for unit in units))\n",
        "unit_encoder = LabelEncoder()\n",
        "unit_encoder.fit(unit_labels)\n",
        "\n",
        "def get_unit_options(entity_name):\n",
        "    return entity_unit_map.get(entity_name, set())\n",
        "\n",
        "def encode_unit(entity_name, unit):\n",
        "    allowed_units = get_unit_options(entity_name)\n",
        "    if unit in allowed_units:\n",
        "        return unit_encoder.transform([unit])[0]\n",
        "    return np.nan\n",
        "\n",
        "train_df['unit_encoded'] = train_df.apply(lambda row: encode_unit(row['entity_name'], row['unit']), axis=1)\n",
        "train_df.dropna(subset=['unit_encoded'], inplace=True)"
      ],
      "metadata": {
        "id": "pv6syZkLS5WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Preprocess images\n",
        "def load_and_preprocess_image(image_path, target_size=(224, 224)):\n",
        "    try:\n",
        "        img = Image.open(image_path).convert('RGB')\n",
        "        img = img.resize(target_size)\n",
        "        img = np.array(img) / 255.0\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {image_path}: {e}\")\n",
        "        return np.zeros((target_size[0], target_size[1], 3))\n",
        "\n",
        "train_images = np.array([load_and_preprocess_image(os.path.join(image_directory, f\"{row['group_id']}_{row['image_name']}\")) for _, row in train_df.iterrows()])\n"
      ],
      "metadata": {
        "id": "o8ggdicSS5Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Preprocess text\n",
        "def preprocess_text(text_data, max_vocab_size=10000, max_seq_length=100):\n",
        "    text_vectorizer = tf.keras.layers.TextVectorization(max_tokens=max_vocab_size, output_sequence_length=max_seq_length)\n",
        "    text_vectorizer.adapt(text_data)\n",
        "    preprocessed_texts = text_vectorizer(text_data)\n",
        "    return text_vectorizer, preprocessed_texts\n",
        "\n",
        "train_texts = train_df['extracted_text'].values\n",
        "text_vectorizer, train_texts_preprocessed = preprocess_text(train_texts)\n"
      ],
      "metadata": {
        "id": "akBk0gc-S5Z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Define the multitask model\n",
        "def build_multitask_model(image_input_shape, text_input_shape, num_units):\n",
        "    image_input = Input(shape=image_input_shape)\n",
        "    resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=image_input_shape)\n",
        "    x = resnet_model(image_input)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    text_input = Input(shape=text_input_shape)\n",
        "    text_features = Embedding(input_dim=10000, output_dim=64, input_length=text_input_shape[0])(text_input)\n",
        "    text_features = GlobalAveragePooling1D()(text_features)\n",
        "\n",
        "    combined_features = Concatenate()([x, text_features])\n",
        "\n",
        "    numeric_output = Dense(1, name='numeric_output')(combined_features)\n",
        "    unit_output = Dense(num_units, activation='softmax', name='unit_output')(combined_features)\n",
        "\n",
        "    model = Model(inputs=[image_input, text_input], outputs=[numeric_output, unit_output])\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss={'numeric_output': 'mean_squared_error', 'unit_output': 'sparse_categorical_crossentropy'},\n",
        "                  metrics={'numeric_output': 'mae', 'unit_output': 'accuracy'})\n",
        "    return model\n",
        "\n",
        "image_input_shape = train_images.shape[1:]\n",
        "text_input_shape = (train_texts_preprocessed.shape[1],)\n",
        "num_units = len(unit_labels)\n",
        "\n",
        "model = build_multitask_model(image_input_shape, text_input_shape, num_units)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S40jiMrPS5cX",
        "outputId": "d8a4c32b-9120-43d4-fff5-e662e01f7350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_entity_encoded = train_df['entity_name_encoded'].values"
      ],
      "metadata": {
        "id": "5w7S9DLSNsbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Define the multitask model\n",
        "def build_multitask_model(image_input_shape, text_input_shape, entity_input_shape, num_units):\n",
        "    image_input = Input(shape=image_input_shape)\n",
        "    resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=image_input_shape)\n",
        "    x = resnet_model(image_input)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    text_input = Input(shape=text_input_shape)\n",
        "    text_features = Embedding(input_dim=10000, output_dim=64, input_length=text_input_shape[0])(text_input)\n",
        "    text_features = GlobalAveragePooling1D()(text_features)\n",
        "\n",
        "    entity_input = Input(shape=(1,))  # Assume entity_name_encoded is a single integer per example\n",
        "    entity_features = Embedding(input_dim=100, output_dim=8)(entity_input)  # Modify input_dim and output_dim as needed\n",
        "    entity_features = Flatten()(entity_features)\n",
        "\n",
        "    combined_features = Concatenate()([x, text_features, entity_features])\n",
        "\n",
        "    numeric_output = Dense(1, name='numeric_output')(combined_features)\n",
        "    unit_output = Dense(num_units, activation='softmax', name='unit_output')(combined_features)\n",
        "\n",
        "    model = Model(inputs=[image_input, text_input, entity_input], outputs=[numeric_output, unit_output])\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss={'numeric_output': 'mean_squared_error', 'unit_output': 'sparse_categorical_crossentropy'},\n",
        "                  metrics={'numeric_output': 'mae', 'unit_output': 'accuracy'})\n",
        "    return model\n",
        "\n",
        "image_input_shape = train_images.shape[1:]\n",
        "text_input_shape = (train_texts_preprocessed.shape[1],)\n",
        "entity_input_shape = (1,)\n",
        "num_units = len(unit_labels)\n",
        "\n",
        "model = build_multitask_model(image_input_shape, text_input_shape, entity_input_shape, num_units)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHNUwRu8NC8i",
        "outputId": "45975488-0ef8-4277-d106-65291fc4fb80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Set up checkpoint and training\n",
        "checkpoint_path = '/content/final_model_1034.keras'\n",
        "checkpoint_callback = ModelCheckpoint(filepath=checkpoint_path, save_best_only=False, monitor='loss', save_freq=20, verbose=1)"
      ],
      "metadata": {
        "id": "j5m6egfzS5d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    [train_images, train_texts_preprocessed, train_entity_encoded],\n",
        "    {'numeric_output': train_df['numeric_value_normalized'].values, 'unit_output': train_df['unit_encoded'].values},\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    callbacks=[checkpoint_callback]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E4qzwXmS5gA",
        "outputId": "eba5cb7e-87d6-4884-c03b-88f158096151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m19/45\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 291ms/step - loss: 7.1599 - numeric_output_mae: 1.2121 - unit_output_accuracy: 0.2286\n",
            "Epoch 1: saving model to /content/final_model_1034.keras\n",
            "\u001b[1m39/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 749ms/step - loss: 5.8691 - numeric_output_mae: 0.9501 - unit_output_accuracy: 0.2527\n",
            "Epoch 1: saving model to /content/final_model_1034.keras\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 2s/step - loss: 5.5489 - numeric_output_mae: 0.8833 - unit_output_accuracy: 0.2588\n",
            "Epoch 2/10\n",
            "\u001b[1m14/45\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 284ms/step - loss: 1.9492 - numeric_output_mae: 0.1008 - unit_output_accuracy: 0.3286\n",
            "Epoch 2: saving model to /content/final_model_1034.keras\n",
            "\u001b[1m34/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m16s\u001b[0m 1s/step - loss: 1.9275 - numeric_output_mae: 0.1002 - unit_output_accuracy: 0.3447\n",
            "Epoch 2: saving model to /content/final_model_1034.keras\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 2s/step - loss: 1.9109 - numeric_output_mae: 0.1010 - unit_output_accuracy: 0.3531\n",
            "Epoch 3/10\n",
            "\u001b[1m 9/45\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 288ms/step - loss: 1.3438 - numeric_output_mae: 0.1177 - unit_output_accuracy: 0.5660\n",
            "Epoch 3: saving model to /content/final_model_1034.keras\n",
            "\u001b[1m29/45\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 1s/step - loss: 1.4310 - numeric_output_mae: 0.1121 - unit_output_accuracy: 0.5298\n",
            "Epoch 3: saving model to /content/final_model_1034.keras\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 2s/step - loss: 1.4514 - numeric_output_mae: 0.1097 - unit_output_accuracy: 0.5248\n",
            "Epoch 4/10\n",
            "\u001b[1m 4/45\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 287ms/step - loss: 1.0204 - numeric_output_mae: 0.1027 - unit_output_accuracy: 0.7188\n",
            "Epoch 4: saving model to /content/final_model_1034.keras\n",
            "\u001b[1m24/45\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 2s/step - loss: 1.0186 - numeric_output_mae: 0.1287 - unit_output_accuracy: 0.7013\n",
            "Epoch 4: saving model to /content/final_model_1034.keras\n",
            "\u001b[1m44/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 2s/step - loss: 0.9921 - numeric_output_mae: 0.1383 - unit_output_accuracy: 0.7010\n",
            "Epoch 4: saving model to /content/final_model_1034.keras\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 3s/step - loss: 0.9911 - numeric_output_mae: 0.1390 - unit_output_accuracy: 0.7011\n",
            "Epoch 5/10\n",
            "\u001b[1m19/45\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 291ms/step - loss: 0.6254 - numeric_output_mae: 0.1735 - unit_output_accuracy: 0.8470\n",
            "Epoch 5: saving model to /content/final_model_1034.keras\n",
            "\u001b[1m39/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m8s\u001b[0m 1s/step - loss: 0.6065 - numeric_output_mae: 0.1799 - unit_output_accuracy: 0.8521\n",
            "Epoch 5: saving model to /content/final_model_1034.keras\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 2s/step - loss: 0.6085 - numeric_output_mae: 0.1804 - unit_output_accuracy: 0.8505\n",
            "Epoch 6/10\n",
            "\u001b[1m14/45\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 294ms/step - loss: 0.4853 - numeric_output_mae: 0.1657 - unit_output_accuracy: 0.9023\n",
            "Epoch 6: saving model to /content/final_model_1034.keras\n",
            "\u001b[1m34/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m15s\u001b[0m 1s/step - loss: 0.4658 - numeric_output_mae: 0.1703 - unit_output_accuracy: 0.8970\n",
            "Epoch 6: saving model to /content/final_model_1034.keras\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 2s/step - loss: 0.4578 - numeric_output_mae: 0.1707 - unit_output_accuracy: 0.8960\n",
            "Epoch 7/10\n",
            "\u001b[1m 9/45\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 287ms/step - loss: 0.2336 - numeric_output_mae: 0.1571 - unit_output_accuracy: 0.9693\n",
            "Epoch 7: saving model to /content/final_model_1034.keras\n",
            "\u001b[1m29/45\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 2s/step - loss: 0.2528 - numeric_output_mae: 0.1664 - unit_output_accuracy: 0.9554\n",
            "Epoch 7: saving model to /content/final_model_1034.keras\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 2s/step - loss: 0.2566 - numeric_output_mae: 0.1669 - unit_output_accuracy: 0.9506\n",
            "Epoch 8/10\n",
            "\u001b[1m 4/45\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 287ms/step - loss: 0.1327 - numeric_output_mae: 0.1729 - unit_output_accuracy: 0.9954\n",
            "Epoch 8: saving model to /content/final_model_1034.keras\n",
            "\u001b[1m24/45\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 2s/step - loss: 0.1824 - numeric_output_mae: 0.1784 - unit_output_accuracy: 0.9778\n",
            "Epoch 8: saving model to /content/final_model_1034.keras\n",
            "\u001b[1m44/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 2s/step - loss: 0.1874 - numeric_output_mae: 0.1766 - unit_output_accuracy: 0.9743\n",
            "Epoch 8: saving model to /content/final_model_1034.keras\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 3s/step - loss: 0.1876 - numeric_output_mae: 0.1763 - unit_output_accuracy: 0.9740\n",
            "Epoch 9/10\n",
            "\u001b[1m19/45\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 293ms/step - loss: 0.1268 - numeric_output_mae: 0.1553 - unit_output_accuracy: 0.9862\n",
            "Epoch 9: saving model to /content/final_model_1034.keras\n",
            "\u001b[1m39/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m7s\u001b[0m 1s/step - loss: 0.1260 - numeric_output_mae: 0.1524 - unit_output_accuracy: 0.9829\n",
            "Epoch 9: saving model to /content/final_model_1034.keras\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 2s/step - loss: 0.1278 - numeric_output_mae: 0.1517 - unit_output_accuracy: 0.9816\n",
            "Epoch 10/10\n",
            "\u001b[1m14/45\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 291ms/step - loss: 0.1275 - numeric_output_mae: 0.1331 - unit_output_accuracy: 0.9665\n",
            "Epoch 10: saving model to /content/final_model_1034.keras\n",
            "\u001b[1m34/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m13s\u001b[0m 1s/step - loss: 0.1194 - numeric_output_mae: 0.1281 - unit_output_accuracy: 0.9732\n",
            "Epoch 10: saving model to /content/final_model_1034.keras\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 2s/step - loss: 0.1149 - numeric_output_mae: 0.1265 - unit_output_accuracy: 0.9754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "test_df = pd.read_csv('/content/test.csv')\n",
        "test = test_df.dropna()\n",
        "test = test.head(1500)"
      ],
      "metadata": {
        "id": "W5nNcSnvMMJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive_image_dir = '/content/test_images_2'\n",
        "os.makedirs(drive_image_dir, exist_ok=True)\n",
        "\n",
        "for i, row in tqdm(test.iterrows(), total=test.shape[0]):\n",
        "    image_url = row['image_link']\n",
        "    # Use group_id to create unique image file names\n",
        "    image_name = image_url.split('/')[-1]  # Extract image name from the URL\n",
        "    image_path = os.path.join(drive_image_dir, f\"{row['group_id']}_{image_name}\")\n",
        "\n",
        "    if not os.path.exists(image_path):  # Check if the image already exists\n",
        "        download_image(image_url, image_path)\n",
        "\n",
        "print(f\"Images saved in: {drive_image_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OMs6TP3MMgW",
        "outputId": "682e63a2-451b-40aa-a39b-1d22a71bb664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1500/1500 [00:28<00:00, 53.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images saved in: /content/test_images_2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory containing the downloaded images\n",
        "image_directory = '/content/test_images_2'\n",
        "\n",
        "# Function to extract text from an image file\n",
        "def extract_text_from_image(group_id, image_name):\n",
        "    filename = f\"{group_id}_{image_name}\"\n",
        "    image_path = os.path.join(image_directory, filename)\n",
        "    result = reader.readtext(image_path)\n",
        "    text = ' '.join([res[1] for res in result])\n",
        "    return text\n",
        "\n",
        "# Extract image names from the image URLs\n",
        "test['image_name'] = test['image_link'].apply(lambda x: x.split('/')[-1])\n",
        "\n",
        "# Extract text from images\n",
        "test['extracted_text'] = test.apply(lambda row: extract_text_from_image(row['group_id'], row['image_name']), axis=1)"
      ],
      "metadata": {
        "id": "fp10hO04MdH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#Initiate encoders\n",
        "group_id_encoder = LabelEncoder()\n",
        "entity_name_encoder = LabelEncoder()\n",
        "test['group_id_encoded'] = group_id_encoder.fit_transform(test['group_id'])\n",
        "test['entity_name_encoded'] = entity_name_encoder.fit_transform(test['entity_name'])"
      ],
      "metadata": {
        "id": "nHyqkmuoMlwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_directory = '/content/test_images'\n",
        "# Load and preprocess all images\n",
        "test_images = np.array([load_and_preprocess_image(os.path.join(image_directory, f\"{row['group_id']}_{row['image_link'].split('/')[-1]}\")) for _, row in test.iterrows()])"
      ],
      "metadata": {
        "id": "nItUX4lpM4VO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts = test['extracted_text'].values"
      ],
      "metadata": {
        "id": "P6P0aUrjM4VP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorizer, test_texts_preprocessed = preprocess_text(test_texts)\n",
        "text_input_shape = (test_texts_preprocessed.shape[1],)"
      ],
      "metadata": {
        "id": "5RZOLDWdNVGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_input_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxS8P7m3UqiW",
        "outputId": "636e537a-ec1b-4a72-fe0d-49406405f583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts_preprocessed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jG4v2sQtUABY",
        "outputId": "1722c365-de97-45c9-cbcb-45cae905708b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(150, 100), dtype=int64, numpy=\n",
              "array([[364, 356, 335, ...,   0,   0,   0],\n",
              "       [  5,  14,  16, ...,   0,   0,   0],\n",
              "       [  5,  14,  16, ...,   0,   0,   0],\n",
              "       ...,\n",
              "       [314, 164, 226, ...,   0,   0,   0],\n",
              "       [115, 104, 117, ...,   0,   0,   0],\n",
              "       [115, 104, 117, ...,   0,   0,   0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_entity_encoded = test['entity_name_encoded'].values"
      ],
      "metadata": {
        "id": "1EJ7X8HsTYBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts_preprocessed = tf.reshape(test_texts_preprocessed, [test_texts_preprocessed.shape[0], 100])"
      ],
      "metadata": {
        "id": "JFQhNsG2TuU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict([test_images, test_texts_preprocessed, test_entity_encoded])\n",
        "\n",
        "# Extract numeric and unit predictions\n",
        "predicted_numeric_values = predictions[0]\n",
        "predicted_units = np.argmax(predictions[1], axis=-1)"
      ],
      "metadata": {
        "id": "6ulxWQF7AQD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f27c5bb-3a73-4c6d-9fe3-d77512490eeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 904ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15Qw_d1EHGJN",
        "outputId": "6653fc80-1c92-40dd-d0bf-5d20dad27bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0.45435172],\n",
            "       [0.45422116],\n",
            "       [0.4559934 ],\n",
            "       [0.45652092],\n",
            "       [0.45617333],\n",
            "       [0.45564583],\n",
            "       [0.45387357],\n",
            "       [0.44267216],\n",
            "       [0.45315883],\n",
            "       [0.4546415 ],\n",
            "       [0.49043813],\n",
            "       [0.47529367],\n",
            "       [0.4681591 ],\n",
            "       [0.4676316 ],\n",
            "       [0.46585938],\n",
            "       [0.46552303],\n",
            "       [0.45234406],\n",
            "       [0.49666563],\n",
            "       [0.5125043 ],\n",
            "       [0.44614315],\n",
            "       [0.43596604],\n",
            "       [0.45569202],\n",
            "       [0.45934695],\n",
            "       [0.44229558],\n",
            "       [0.44459534],\n",
            "       [0.466106  ],\n",
            "       [0.46840575],\n",
            "       [0.46242484],\n",
            "       [0.43848684],\n",
            "       [0.4792185 ],\n",
            "       [0.47994784],\n",
            "       [0.47527254],\n",
            "       [0.47265702],\n",
            "       [0.47495678],\n",
            "       [0.47442928],\n",
            "       [0.49886468],\n",
            "       [0.45455104],\n",
            "       [0.42755786],\n",
            "       [0.49391904],\n",
            "       [0.46545509],\n",
            "       [0.46315533],\n",
            "       [0.4554018 ],\n",
            "       [0.46420962],\n",
            "       [0.46598187],\n",
            "       [0.44147083],\n",
            "       [0.47945955],\n",
            "       [0.47893205],\n",
            "       [0.47715977],\n",
            "       [0.4655804 ],\n",
            "       [0.4678802 ],\n",
            "       [0.46735266],\n",
            "       [0.45462492],\n",
            "       [0.4704637 ],\n",
            "       [0.4726765 ],\n",
            "       [0.47214898],\n",
            "       [0.3931451 ],\n",
            "       [0.46539298],\n",
            "       [0.46534055],\n",
            "       [0.4630408 ],\n",
            "       [0.5122966 ],\n",
            "       [0.44593546],\n",
            "       [0.49074835],\n",
            "       [0.4557744 ],\n",
            "       [0.4563019 ],\n",
            "       [0.45400214],\n",
            "       [0.51680285],\n",
            "       [0.5261703 ],\n",
            "       [0.4205943 ],\n",
            "       [0.46518192],\n",
            "       [0.46695417],\n",
            "       [0.46748167],\n",
            "       [0.4630953 ],\n",
            "       [0.46486756],\n",
            "       [0.4653951 ],\n",
            "       [0.52433836],\n",
            "       [0.4579772 ],\n",
            "       [0.45971856],\n",
            "       [0.46024606],\n",
            "       [0.49018845],\n",
            "       [0.46474057],\n",
            "       [0.49180657],\n",
            "       [0.51371056],\n",
            "       [0.5571317 ],\n",
            "       [0.4998574 ],\n",
            "       [0.45177945],\n",
            "       [0.4500072 ],\n",
            "       [0.45230696],\n",
            "       [0.6096969 ],\n",
            "       [0.54333574],\n",
            "       [0.5938582 ],\n",
            "       [0.4688391 ],\n",
            "       [0.53309464],\n",
            "       [0.45907208],\n",
            "       [0.39177784],\n",
            "       [0.48084855],\n",
            "       [0.46881878],\n",
            "       [0.46704653],\n",
            "       [0.47310272],\n",
            "       [0.4655675 ],\n",
            "       [0.46639886],\n",
            "       [0.47279012],\n",
            "       [0.34565368],\n",
            "       [0.50312215],\n",
            "       [0.3974026 ],\n",
            "       [0.46742508],\n",
            "       [0.4576237 ],\n",
            "       [0.4558514 ],\n",
            "       [0.51957744],\n",
            "       [0.48870558],\n",
            "       [0.45870513],\n",
            "       [0.46061733],\n",
            "       [0.46059504],\n",
            "       [0.64546216],\n",
            "       [0.6296234 ],\n",
            "       [0.60559267],\n",
            "       [0.6214314 ],\n",
            "       [0.47560945],\n",
            "       [0.49027625],\n",
            "       [0.48455176],\n",
            "       [0.4868515 ],\n",
            "       [0.47691104],\n",
            "       [0.46968088],\n",
            "       [0.39145508],\n",
            "       [0.49809286],\n",
            "       [0.48038062],\n",
            "       [0.5267152 ],\n",
            "       [0.4761928 ],\n",
            "       [0.414696  ],\n",
            "       [0.4594898 ],\n",
            "       [0.5647985 ],\n",
            "       [0.47024575],\n",
            "       [0.45764557],\n",
            "       [0.45941782],\n",
            "       [0.45994532],\n",
            "       [0.5081303 ],\n",
            "       [0.4460877 ],\n",
            "       [0.4466152 ],\n",
            "       [0.4668409 ],\n",
            "       [0.4657922 ],\n",
            "       [0.46586004],\n",
            "       [0.46815982],\n",
            "       [0.4638491 ],\n",
            "       [0.49945256],\n",
            "       [0.49715278],\n",
            "       [0.4312861 ],\n",
            "       [0.4307586 ],\n",
            "       [0.42898634],\n",
            "       [0.38002107],\n",
            "       [0.48526087],\n",
            "       [0.48756066]], dtype=float32), array([[1.8329001e-06, 7.4842197e-01, 2.6800840e-06, ..., 1.2977362e-01,\n",
            "        3.1612720e-02, 7.5050998e-06],\n",
            "       [2.6125679e-06, 6.5449202e-01, 3.4965890e-06, ..., 1.6361904e-01,\n",
            "        4.7846925e-02, 1.0128152e-05],\n",
            "       [2.8043683e-06, 6.2789905e-01, 3.7112327e-06, ..., 1.7445916e-01,\n",
            "        5.0971381e-02, 1.0796457e-05],\n",
            "       ...,\n",
            "       [3.7333797e-07, 7.9334784e-01, 7.5725666e-07, ..., 1.6493054e-01,\n",
            "        1.7959699e-02, 1.6902201e-06],\n",
            "       [3.1118209e-06, 6.3517159e-01, 3.7815478e-06, ..., 1.7065001e-01,\n",
            "        5.6326315e-02, 1.0484154e-05],\n",
            "       [3.4252103e-06, 5.8761466e-01, 4.0760651e-06, ..., 1.9483495e-01,\n",
            "        6.2240440e-02, 1.1357541e-05]], dtype=float32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnFL67qgHL9l",
        "outputId": "c4462c13-b5a0-47eb-e5db-0dc33313e288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.8329001e-06, 7.4842197e-01, 2.6800840e-06, ..., 1.2977362e-01,\n",
              "        3.1612720e-02, 7.5050998e-06],\n",
              "       [2.6125679e-06, 6.5449202e-01, 3.4965890e-06, ..., 1.6361904e-01,\n",
              "        4.7846925e-02, 1.0128152e-05],\n",
              "       [2.8043683e-06, 6.2789905e-01, 3.7112327e-06, ..., 1.7445916e-01,\n",
              "        5.0971381e-02, 1.0796457e-05],\n",
              "       ...,\n",
              "       [3.7333797e-07, 7.9334784e-01, 7.5725666e-07, ..., 1.6493054e-01,\n",
              "        1.7959699e-02, 1.6902201e-06],\n",
              "       [3.1118209e-06, 6.3517159e-01, 3.7815478e-06, ..., 1.7065001e-01,\n",
              "        5.6326315e-02, 1.0484154e-05],\n",
              "       [3.4252103e-06, 5.8761466e-01, 4.0760651e-06, ..., 1.9483495e-01,\n",
              "        6.2240440e-02, 1.1357541e-05]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_values = np.argmax(predictions[1], axis=1)\n",
        "print(max_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjVkzbasH06n",
        "outputId": "7af1876b-adb6-4295-f52c-8dc97fac5f22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1  1  1  1  1  1  1  1  1  1  9  1  1  1  1  1  1  9  1 28  1  1  1  1\n",
            "  1  1  1  1  1  1  1  1  1  1  1 28  1 28  1  1  1  1  1  1  1  1  1  1\n",
            "  1  1  1  9  1  1  1  1  1  1  1  1 28  9  1  1  1  1  9  1  1  1  1  1\n",
            "  1  1  1 28  1  1  1  1  9  1  1  1  1  1  1 29 28  9  1  1  1  1  9  1\n",
            "  1  1  1  1  1  1  1  1  1  1  1  1 28  1  1  1  1  1  9  1  9  1  1  1\n",
            " 28  1  9  1  1  1 28  1  1  9  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
            "  1  1  1  1  1  1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unit_labels[1], unit_labels[9], unit_labels[28], unit_labels[29]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdDkGK68I6ou",
        "outputId": "3b6cf169-8d64-4c35-e1dc-aa2371b4bb59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('centimetre', 'gram', 'volt', 'watt')"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_numeric_values = predictions[0]\n",
        "predicted_units = predictions[1]"
      ],
      "metadata": {
        "id": "hjLNHSnFVqhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_numeric_values_denorm = numeric_scaler.inverse_transform(predicted_numeric_values)\n",
        "\n",
        "decoded_units = unit_encoder.inverse_transform(predicted_units)"
      ],
      "metadata": {
        "id": "3-XBwM2bJ2x5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}